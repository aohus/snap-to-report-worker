#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
new_deep_clusterer.py

ê³µì› ì‚¬ì§„ ìë™ ë¶„ë¥˜ê¸° (ê³ ì •ë°€ ë”¥ëŸ¬ë‹ + ê¸°í•˜ ê²€ì¦ ë²„ì „)

ê¸°ì¡´ deep_clusterer.DeepClusterer ì™€ ì™„ì „íˆ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ ì§€í•˜ë©´ì„œ,
ë‹¤ìŒê³¼ ê°™ì€ ì ì„ ê°•í™”í•œ ë²„ì „ì…ë‹ˆë‹¤.

- CLIP + EfficientNet + ViT + ì „í†µ íŠ¹ì§•ì„ ê²°í•©í•œ ê³ ì°¨ì› íŠ¹ì§• ë²¡í„°
- k-NN ê¸°ë°˜ ì „ì—­ ì„ë² ë”© ìœ ì‚¬ë„
- SIFT + RANSAC ë¥¼ ì´ìš©í•œ ê¸°í•˜í•™ì  ê²€ì¦(ê°™ì€ ì¥ì†Œ ì—¬ë¶€ íŒë³„ ê°•í™”)
- ì—°ê²° ìš”ì†Œ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ + í’ˆì§ˆ ìŠ¤ì½”ì–´(ìœ ì‚¬ë„ Ã— ê°œìˆ˜)
"""

from __future__ import annotations

import hashlib
import json
import logging
import os
import pickle
import shutil
import time
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
from PIL import Image, ImageDraw
from sklearn.neighbors import NearestNeighbors
from tqdm import tqdm

warnings.filterwarnings("ignore")

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# ìœ í‹¸: ë¡œì»¬ íŠ¹ì§• + RANSAC ê¸°ë°˜ ê¸°í•˜í•™ì  ìœ ì‚¬ë„
# ---------------------------------------------------------------------------

try:
    import cv2
except Exception:  # pragma: no cover - í™˜ê²½ì— ë”°ë¼ cv2ê°€ ì—†ì„ ìˆ˜ë„ ìˆìŒ
    cv2 = None  # type: ignore


class LocalGeometryMatcher:
    """
    ë‘ ì´ë¯¸ì§€ ì‚¬ì´ì˜ ê¸°í•˜í•™ì  ì¼ê´€ì„±ì„ SIFT + RANSAC ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë„ìš°ë¯¸ í´ë˜ìŠ¤.
    """

    def __init__(
        self,
        max_features: int = 2000,
        ratio_thresh: float = 0.5,
        ransac_reproj_thresh: float = 5.0,
        min_good_matches: int = 3,
    ) -> None:
        self.enabled = cv2 is not None
        if not self.enabled:
            logger.warning(
                "âš ï¸ OpenCV(cv2)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°í•˜í•™ì  ê²€ì¦ ë‹¨ê³„ëŠ” ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. "
                "pip install opencv-python-headless ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            self.detector = None
        else:
            # SIFTëŠ” opencv-contrib-python í•„ìš”
            try:
                self.detector = cv2.SIFT_create(nfeatures=max_features)  # type: ignore[attr-defined]
            except Exception:
                self.detector = None
                self.enabled = False
                logger.warning(
                    "âš ï¸ SIFT ìƒì„± ì‹¤íŒ¨. opencv-contrib-python ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. "
                    "ê¸°í•˜í•™ì  ê²€ì¦ ë‹¨ê³„ëŠ” ë¹„í™œì„±í™”ë©ë‹ˆë‹¤."
                )

        self.ratio_thresh = ratio_thresh
        self.ransac_reproj_thresh = ransac_reproj_thresh
        self.min_good_matches = min_good_matches

    def _load_gray(self, path: Path) -> Optional[np.ndarray]:
        if not self.enabled or self.detector is None:
            return None
        img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)  # type: ignore[operator]
        if img is None:
            return None
        return img

    def geo_score(self, path1: Path, path2: Path) -> float:
        """
        0.0 ~ 1.0 ì‚¬ì´ì˜ ê¸°í•˜í•™ì  ì¼ê´€ì„± ì ìˆ˜.
        - 0.0 ì— ê°€ê¹Œìš¸ìˆ˜ë¡ êµ¬ì¡°ê°€ ë‹¤ë¥´ê±°ë‚˜ ë§¤ì¹­ ì‹¤íŒ¨
        - 1.0 ì— ê°€ê¹Œìš¸ìˆ˜ë¡ êµ¬ì¡°ê°€ ìƒë‹¹íˆ ì¼ì¹˜
        """
        if not self.enabled or self.detector is None:
            return 1.0  # ê¸°í•˜ ê²€ì¦ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” í™˜ê²½ì—ì„œëŠ” í•­ìƒ í†µê³¼

        img1 = self._load_gray(path1)
        img2 = self._load_gray(path2)
        if img1 is None or img2 is None:
            return 0.0

        keypoints1, desc1 = self.detector.detectAndCompute(img1, None)
        keypoints2, desc2 = self.detector.detectAndCompute(img2, None)
        if desc1 is None or desc2 is None:
            return 0.0

        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)  # type: ignore[attr-defined]
        matches = bf.knnMatch(desc1, desc2, k=2)  # type: ignore[attr-defined]

        good = []
        for m, n in matches:
            if m.distance < self.ratio_thresh * n.distance:
                good.append(m)

        if len(good) < self.min_good_matches:
            return 0.0

        pts1 = np.float32([keypoints1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
        pts2 = np.float32([keypoints2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

        H, mask = cv2.findHomography(  # type: ignore[attr-defined]
            pts1, pts2, cv2.RANSAC, ransacReprojThreshold=self.ransac_reproj_thresh  # type: ignore[attr-defined]
        )
        if H is None or mask is None:
            return 0.0

        inliers = mask.ravel().sum()
        total = len(good)
        if total == 0:
            return 0.0
        score = float(inliers) / float(total)
        return max(0.0, min(1.0, score))


# ---------------------------------------------------------------------------
# ì‚¬ëŒ ê°ì§€ìš© DETR ë˜í¼ (ê¸°ì¡´ deep_clusterer ì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)
# ---------------------------------------------------------------------------

try:
    from transformers import DetrForObjectDetection, DetrImageProcessor
except Exception:  # pragma: no cover
    DetrForObjectDetection = None  # type: ignore
    DetrImageProcessor = None  # type: ignore


class PeopleDetector:
    def __init__(self, processor, model, device):
        self.processor = processor
        self.model = model
        self.device = device

    def __call__(self, images, return_tensors: str = "pt"):
        inputs = self.processor(images=images, return_tensors=return_tensors)
        import torch  # ì§€ì—° ì„í¬íŠ¸

        inputs = {
            k: v.to(self.device) if isinstance(v, torch.Tensor) else v
            for k, v in inputs.items()
        }
        return inputs

    def post_process_object_detection(self, outputs, **kwargs):
        return self.processor.post_process_object_detection(outputs, **kwargs)


def create_people_detector(device):
    """ì‚¬ëŒ ê°ì§€ ëª¨ë¸ ì´ˆê¸°í™” (ì—†ìœ¼ë©´ None ë°˜í™˜)."""
    if DetrForObjectDetection is None or DetrImageProcessor is None:
        logger.warning(
            "âš ï¸ transformers / DETR ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ëŒ ì œê±° ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤."
        )
        return None

    try:
        logger.info("ğŸ§ ì‚¬ëŒ ê°ì§€ ëª¨ë¸ ë¡œë”© ì¤‘ (DETR ResNet-50)...")
        processor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50", use_fast=True)
        model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
        import torch  # ì§€ì—° ì„í¬íŠ¸

        model = model.to(device)
        model.eval()
        return PeopleDetector(processor, model, device)
    except Exception as e:  # pragma: no cover - í™˜ê²½ ì˜ì¡´
        logger.warning(f"âš ï¸ ì‚¬ëŒ ê°ì§€ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


# ---------------------------------------------------------------------------
# ë”¥ëŸ¬ë‹ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ëŸ¬ (ê¸°ì¡´ DeepClusterer ì™€ ë™ì¼í•œ ì´ë¦„ / ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜)
# ---------------------------------------------------------------------------


class DeepClusterer:
    def __init__(
        self,
        input_path,
        similarity_threshold: float = 0.6,
        use_cache: bool = True,
        remove_people: bool = True,
    ):
        """
        Args:
            input_path: ì´ë¯¸ì§€ë“¤ì´ ë“¤ì–´ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œ
            similarity_threshold: ì „ì—­ ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’ (0~1)
            use_cache: íŠ¹ì§• ë²¡í„° ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            remove_people: ì‚¬ëŒ ì˜ì—­ ë§ˆìŠ¤í‚¹ ì—¬ë¶€
        """
        from pathlib import Path as _Path

        import torch

        self.input_path = _Path(input_path)
        self.output_path = self.input_path / "advanced"
        self.cache_dir = self.input_path / ".photo_cache"
        self.similarity_threshold = similarity_threshold
        self.use_cache = use_cache
        self.remove_people = remove_people

        self.photos: List[_Path] = []
        self.groups: List[Dict] = []

        self.dim_clip = 512
        self.dim_efficientnet = 1792
        self.dim_vit = 768
        self.dim_traditional = 128

        if self.use_cache:
            self.cache_dir.mkdir(exist_ok=True)
            logger.info(f"ğŸ’¾ Cache directory: {self.cache_dir}")

        self.image_extensions = {".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".webp"}
        if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            # Mac (Apple Silicon) + Metal GPU
            self.device = torch.device("mps")
            logger.info("ğŸ”§ Using device: mps (Apple Silicon GPU)")
        elif torch.cuda.is_available():
            # NVIDIA GPU (Linux/Windows ë“±)
            self.device = torch.device("cuda")
            logger.info("ğŸ”§ Using device: cuda (NVIDIA GPU)")
        else:
            self.device = torch.device("cpu")
            logger.info("ğŸ”§ Using device: cpu")
            logger.info(f"ğŸ”§ Using device: {self.device}")

        # ìºì‹œ í†µê³„
        if self.use_cache:
            self.cache_stats: Dict[str, int] = {"hits": 0, "misses": 0}

        # ëª¨ë¸ë“¤
        self.clip_model = None
        self.clip_preprocess = None
        self.efficientnet = None
        self.vit = None
        self.people_detector: Optional[PeopleDetector] = None

        # ê¸°í•˜ ê²€ì¦ê¸°
        self.geo_matcher = LocalGeometryMatcher()

        self.setup_models()

    # ------------------------------------------------------------------
    # ëª¨ë¸ ë¡œë”©
    # ------------------------------------------------------------------
    def setup_models(self):
        """CLIP + EfficientNet + ViT ëª¨ë¸ë“¤ì„ ì´ˆê¸°í™”í•œë‹¤."""
        logger.info("ğŸ¤– Loading vision models...")

        try:
            import open_clip
            import timm
            import torch

            # ì‚¬ëŒ ì œê±° ëª¨ë¸
            if self.remove_people:
                self.people_detector = create_people_detector(self.device)

            # OpenCLIP (ViT-B-32)
            logger.info("   ğŸ“¥ Loading OpenCLIP ViT-B-32...")
            self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(
                "ViT-B-32", pretrained="openai"
            )
            self.clip_model.to(self.device)
            self.clip_model.eval()

            # EfficientNet (feature extractor)
            logger.info("   ğŸ“¥ Loading EfficientNet-B4...")
            self.efficientnet = timm.create_model(
                "efficientnet_b4", pretrained=True, num_classes=0
            ).to(self.device)
            self.efficientnet.eval()

            # ViT (feature extractor)
            logger.info("   ğŸ“¥ Loading ViT-B-16...")
            self.vit = timm.create_model(
                "vit_base_patch16_224", pretrained=True, num_classes=0
            ).to(self.device)
            self.vit.eval()

            logger.info("âœ… All models loaded successfully!")

        except Exception as e:  # pragma: no cover - ì„¤ì¹˜ í™˜ê²½ì— ë”°ë¼
            logger.error(f"âŒ Model loading failed: {e}")
            logger.info(
                "ğŸ’¡ Please install required libraries with:\n"
                "   pip install torch torchvision timm open_clip_torch transformers opencv-python-headless"
            )
            raise

    # ------------------------------------------------------------------
    # ìºì‹œ ê´€ë ¨
    # ------------------------------------------------------------------
    def get_file_hash(self, file_path: Path) -> str:
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def get_cache_path(self, file_path: Path, feature_type: str) -> Path:
        file_hash = self.get_file_hash(file_path)
        cache_filename = f"{file_hash}_{feature_type}.pkl"
        return self.cache_dir / cache_filename

    def load_from_cache(self, file_path: Path, feature_type: str):
        if not self.use_cache:
            return None

        cache_path = self.get_cache_path(file_path, feature_type)
        if cache_path.exists():
            try:
                with open(cache_path, "rb") as f:
                    features = pickle.load(f)
                if self.use_cache:
                    self.cache_stats["hits"] += 1
                return features
            except Exception as e:
                logger.warning(f"âš ï¸ Cache load failed ({cache_path}): {e}")
        if self.use_cache:
            self.cache_stats["misses"] += 1
        return None

    def save_to_cache(self, file_path: Path, feature_type: str, features):
        if not self.use_cache:
            return
        cache_path = self.get_cache_path(file_path, feature_type)
        try:
            with open(cache_path, "wb") as f:
                pickle.dump(features, f)
        except Exception as e:
            logger.warning(f"âš ï¸ Cache save failed: {e}")

    def clear_cache(self):
        if self.cache_dir.exists():
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir(exist_ok=True)
            logger.info("ğŸ—‘ï¸ Cache cleared.")

    # ------------------------------------------------------------------
    # ì´ë¯¸ì§€ ë¡œë”© ë° ì „ì²˜ë¦¬
    # ------------------------------------------------------------------
    def load_photos(self) -> List[Path]:
        """ì…ë ¥ í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ì„ ìˆ˜ì§‘í•œë‹¤."""
        if self.photos:
            # cluster(photo_paths) ë¡œ ì „ë‹¬ëœ ë¦¬ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
            logger.info(f"ğŸ“· Using {len(self.photos)} pre-specified photo paths.")
            return self.photos

        photo_files: List[Path] = []
        for root, _, files in os.walk(self.input_path):
            root_path = Path(root)
            # ê²°ê³¼ í´ë”/ìºì‹œ í´ë”ëŠ” ì œì™¸
            if root_path == self.output_path or self.cache_dir in root_path.parents:
                continue
            for name in files:
                ext = Path(name).suffix.lower()
                if ext in self.image_extensions:
                    photo_files.append(root_path / name)

        photo_files.sort()
        logger.info(f"ğŸ“· Found {len(photo_files)} image files.")
        return photo_files

    # ------------------------------------------------------------------
    # ì‚¬ëŒ ì œê±° (ì„ íƒì )
    # ------------------------------------------------------------------
    def mask_people(self, image: Image.Image) -> Image.Image:
        """
        DETR ê¸°ë°˜ìœ¼ë¡œ ì‚¬ëŒ ë°•ìŠ¤ë¥¼ ì°¾ì•„ íë¦¬ê²Œ ì²˜ë¦¬.
        DETR ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜.
        """
        if not self.remove_people or self.people_detector is None:
            return image

        try:
            import torch

            inputs = self.people_detector([image], return_tensors="pt")
            outputs = self.people_detector.model(**inputs)
            target_sizes = torch.tensor([[image.height, image.width]]).to(self.device)
            results = self.people_detector.post_process_object_detection(
                outputs, target_sizes=target_sizes
            )[0]

            person_boxes: List[List[float]] = []
            for score, label, box in zip(
                results["scores"], results["labels"], results["boxes"]
            ):
                if float(score) < 0.8:
                    continue
                # COCO dataset ì—ì„œ person class id == 1
                if int(label) == 1:
                    person_boxes.append([float(x) for x in box.tolist()])

            if not person_boxes:
                return image

            img_np = np.array(image).copy()
            for x_min, y_min, x_max, y_max in person_boxes:
                x_min_i = max(0, int(x_min))
                y_min_i = max(0, int(y_min))
                x_max_i = min(img_np.shape[1], int(x_max))
                y_max_i = min(img_np.shape[0], int(y_max))

                roi = img_np[y_min_i:y_max_i, x_min_i:x_max_i]
                if roi.size == 0:
                    continue
                roi_blur = cv2.GaussianBlur(roi, (21, 21), 0) if cv2 is not None else roi
                img_np[y_min_i:y_max_i, x_min_i:x_max_i] = roi_blur

            return Image.fromarray(img_np)
        except Exception as e:  # pragma: no cover
            logger.warning(f"âš ï¸ Failed to mask people: {e}")
            return image

    # ------------------------------------------------------------------
    # íŠ¹ì§• ì¶”ì¶œ
    # ------------------------------------------------------------------
    def extract_clip_features(self, image: Image.Image, file_path: Path) -> Optional[np.ndarray]:
        cached = self.load_from_cache(file_path, "clip")
        if cached is not None:
            return cached

        if self.clip_model is None or self.clip_preprocess is None:
            return None

        import torch

        with torch.no_grad():
            image_input = self.clip_preprocess(image).unsqueeze(0).to(self.device)
            features = self.clip_model.encode_image(image_input)
            features = features / features.norm(dim=-1, keepdim=True)
            feat_np: np.ndarray = features.cpu().numpy().flatten()
            self.save_to_cache(file_path, "clip", feat_np)
            return feat_np

    def extract_efficientnet_features(
        self, image: Image.Image, file_path: Path
    ) -> Optional[np.ndarray]:
        cached = self.load_from_cache(file_path, "efficientnet")
        if cached is not None:
            return cached
        if self.efficientnet is None:
            return None

        import torch
        import torchvision.transforms as T

        preprocess = T.Compose(
            [
                T.Resize((380, 380)),
                T.ToTensor(),
                T.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225],
                ),
            ]
        )
        img_tensor = preprocess(image).unsqueeze(0).to(self.device)

        with torch.no_grad():
            feats = self.efficientnet(img_tensor)
            feat_np: np.ndarray = feats.cpu().numpy().flatten()
            self.save_to_cache(file_path, "efficientnet", feat_np)
            return feat_np

    def extract_vit_features(self, image: Image.Image, file_path: Path) -> Optional[np.ndarray]:
        cached = self.load_from_cache(file_path, "vit")
        if cached is not None:
            return cached
        if self.vit is None:
            return None

        import torch
        import torchvision.transforms as T

        preprocess = T.Compose(
            [
                T.Resize((224, 224)),
                T.ToTensor(),
                T.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225],
                ),
            ]
        )
        img_tensor = preprocess(image).unsqueeze(0).to(self.device)

        with torch.no_grad():
            feats = self.vit(img_tensor)
            feat_np: np.ndarray = feats.cpu().numpy().flatten()
            self.save_to_cache(file_path, "vit", feat_np)
            return feat_np

    def extract_traditional_features(self, image_path: Path) -> Optional[np.ndarray]:
        """
        ìƒ‰ íˆìŠ¤í† ê·¸ë¨ + LBP ê¸°ë°˜ ê°„ë‹¨í•œ ì „í†µ íŠ¹ì§•.
        """
        cached = self.load_from_cache(image_path, "traditional")
        if cached is not None:
            return cached

        try:
            img = Image.open(image_path).convert("RGB")
            img_np = np.array(img)

            # ìƒ‰ íˆìŠ¤í† ê·¸ë¨ (ê° ì±„ë„ 32-bin)
            hist_features: List[float] = []
            for ch in range(3):
                hist = cv2.calcHist(  # type: ignore[operator]
                    [img_np], [ch], None, [32], [0, 256]
                )
                hist = cv2.normalize(hist, hist).flatten()  # type: ignore[operator]
                hist_features.extend(hist.tolist())

            # LBP
            gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY) if cv2 is not None else np.array(
                img.convert("L")
            )  # type: ignore[operator]
            lbp = self.calculate_lbp(gray, radius=1, n_points=8)
            lbp_hist, _ = np.histogram(lbp.ravel(), bins=32, range=(0, 256), density=True)
            hist_features.extend(lbp_hist.flatten().tolist())

            feat_np = np.array(hist_features, dtype=np.float32)
            self.save_to_cache(image_path, "traditional", feat_np)
            return feat_np
        except Exception as e:  # pragma: no cover
            logger.warning(f"âš ï¸ Failed to extract traditional features ({image_path}): {e}")
            return None

    def calculate_lbp(self, image: np.ndarray, radius: int = 1, n_points: int = 8) -> np.ndarray:
        rows, cols = image.shape
        lbp = np.zeros((rows, cols), dtype=np.uint8)

        for i in range(radius, rows - radius):
            for j in range(radius, cols - radius):
                center = image[i, j]
                code = 0
                for p in range(n_points):
                    theta = 2.0 * np.pi * p / n_points
                    y = i + int(round(radius * np.sin(theta)))
                    x = j + int(round(radius * np.cos(theta)))
                    neighbor = image[y, x]
                    code |= (1 << p) if neighbor > center else 0
                lbp[i, j] = code
        return lbp

    # ------------------------------------------------------------------
    # ë”¥ íŠ¹ì§• ê²°í•©
    # ------------------------------------------------------------------
    def extract_deep_features(
        self, image_path: Path
    ) -> Tuple[Optional[np.ndarray], Optional[Dict[str, np.ndarray]]]:
        """
        í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ëŒ€í•´ CLIP / EfficientNet / ViT / ì „í†µ íŠ¹ì§•ì„ ëª¨ë‘ ì¶”ì¶œí•˜ê³ 
        4ê°œ blockì„ í•­ìƒ ê°™ì€ ê¸¸ì´ë¡œ ì´ì–´ë¶™ì¸ í•˜ë‚˜ì˜ ë²¡í„°ë¥¼ ë°˜í™˜.
        - ì–´ë–¤ blockì´ ì—†ìœ¼ë©´ í•´ë‹¹ êµ¬ê°„ì€ 0ë²¡í„°ë¡œ ì±„ì›€.
        """
        cached = self.load_from_cache(image_path, "combined")
        if cached is not None:
            # combined ë§Œ ìºì‹œëœ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ dict ëŠ” None ìœ¼ë¡œ ë‘”ë‹¤
            return cached, None

        try:
            img = Image.open(image_path).convert("RGB")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to open image {image_path}: {e}")
            return None, None

        # ì‚¬ëŒ ì œê±° (ì„ íƒ)
        img_for_features = self.mask_people(img)

        clip_f = self.extract_clip_features(img_for_features, image_path)
        eff_f = self.extract_efficientnet_features(img_for_features, image_path)
        # vit_f = self.extract_vit_features(img_for_features, image_path)
        # trad_f = self.extract_traditional_features(image_path)

        vit_f = None
        trad_f = None
        
        # 4ê°œ ë‹¤ ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ ë²„ë¦¼
        if all(f is None for f in (clip_f, eff_f, vit_f, trad_f)):
            return None, None

        features_dict: Dict[str, np.ndarray] = {}

        def _norm_or_none(x: Optional[np.ndarray]) -> Optional[np.ndarray]:
            if x is None:
                return None
            norm = np.linalg.norm(x)
            if norm == 0:
                return None
            return x / norm

        clip_n = _norm_or_none(clip_f)
        eff_n = _norm_or_none(eff_f)
        vit_n = _norm_or_none(vit_f)
        trad_n = _norm_or_none(trad_f)

        # ê° blockì„ í•­ìƒ ê°™ì€ ê¸¸ì´ë¡œ ì¤€ë¹„ (ì—†ìœ¼ë©´ 0ë²¡í„°)
        parts: List[np.ndarray] = []

        # CLIP
        if clip_n is not None:
            vec_clip = (clip_n * 0.6).astype(np.float32)
            features_dict["clip"] = clip_n
        else:
            vec_clip = np.zeros(self.dim_clip, dtype=np.float32)
        parts.append(vec_clip)

        # EfficientNet
        if eff_n is not None:
            vec_eff = (eff_n * 0.4).astype(np.float32)
            features_dict["efficientnet"] = eff_n
        else:
            vec_eff = np.zeros(self.dim_efficientnet, dtype=np.float32)
        parts.append(vec_eff)

        # ViT
        if vit_n is not None:
            vec_vit = (vit_n * 0.20).astype(np.float32)
            features_dict["vit"] = vit_n
        else:
            vec_vit = np.zeros(self.dim_vit, dtype=np.float32)
        parts.append(vec_vit)

        # ì „í†µ íŠ¹ì§•
        if trad_n is not None:
            vec_trad = (trad_n * 0.05).astype(np.float32)
            features_dict["traditional"] = trad_n
        else:
            vec_trad = np.zeros(self.dim_traditional, dtype=np.float32)
        parts.append(vec_trad)

        # ì´ì œ í•­ìƒ ê°™ì€ ê¸¸ì´: 512 + 1792 + 768 + 128 = 3200
        combined = np.concatenate(parts).astype(np.float32)

        self.save_to_cache(image_path, "combined", combined)
        return combined, features_dict

    # ------------------------------------------------------------------
    # ê³ ê¸‰ í´ëŸ¬ìŠ¤í„°ë§ (ì „ì—­ ì„ë² ë”© + ê¸°í•˜ ê²€ì¦ + ê·¸ë˜í”„ ì—°ê²°ìš”ì†Œ)
    # ------------------------------------------------------------------
    def _connected_components(self, n: int, edges: List[Tuple[int, int]]) -> List[int]:
        if n == 0:
            return []
        parent = list(range(n))

        def find(x: int) -> int:
            while parent[x] != x:
                parent[x] = parent[parent[x]]
                x = parent[x]
            return x

        def union(a: int, b: int) -> None:
            ra, rb = find(a), find(b)
            if ra != rb:
                parent[rb] = ra

        for i, j in edges:
            union(i, j)

        roots = [find(i) for i in range(n)]
        root_to_label: Dict[int, int] = {}
        next_label = 0
        labels: List[int] = []
        for r in roots:
            if r not in root_to_label:
                root_to_label[r] = next_label
                next_label += 1
            labels.append(root_to_label[r])
        return labels

    def advanced_clustering(self, features_array: np.ndarray, photo_files: List[Path]) -> List[Dict]:
        """
        ê¸°ì¡´ deep_clusterer ì˜ advanced_clustering ê³¼ ë™ì¼í•œ ì—­í• ì„ ìˆ˜í–‰í•˜ë˜,
        - ì „ì—­ ì„ë² ë”© ê¸°ë°˜ k-NN ê·¸ë˜í”„
        - SIFT + RANSAC ê¸°í•˜ ê²€ì¦
        - ì—°ê²° ìš”ì†Œ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        ì„ ì‚¬ìš©í•˜ì—¬ ê°™ì€ ì¥ì†Œ ì‚¬ì§„ì„ ë”ìš± ì •ë°€í•˜ê²Œ ë¬¶ëŠ”ë‹¤.
        """
        n, d = features_array.shape
        if n == 0:
            return []

        logger.info("ğŸ“Œ Building k-NN graph in feature space...")
        k = min(max(10, int(np.sqrt(n)) + 1), n)
        nn = NearestNeighbors(n_neighbors=k, metric="cosine")
        nn.fit(features_array)
        distances, indices = nn.kneighbors(features_array)

        edges: List[Tuple[int, int]] = []
        geo_threshold = 0.2  # ê¸°í•˜í•™ ì¼ê´€ì„± ìµœì†Œ ë¹„ìœ¨

        for i in range(n):
            for dist, j in zip(distances[i][1:], indices[i][1:]):  # [0] ì€ ìê¸° ìì‹ 
                sim = 1.0 - float(dist)  # cosine distance -> similarity
                if sim < self.similarity_threshold:
                    continue

                # ê¸°í•˜ ê²€ì¦
                score_geo = self.geo_matcher.geo_score(photo_files[i], photo_files[j])
                if score_geo < geo_threshold:
                    continue

                edges.append((i, j))

        logger.info(f"ğŸ”— Retained {len(edges)} edges after geometric verification.")

        if not edges:
            # ì—£ì§€ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê° ì´ë¯¸ì§€ë¥¼ ë³„ë„ ê·¸ë£¹ìœ¼ë¡œ
            groups: List[Dict] = []
            for idx, p in enumerate(photo_files):
                groups.append(
                    {
                        "id": idx,
                        "photos": [p],
                        "count": 1,
                        "avg_similarity": 1.0,
                        "quality_score": 1.0,
                    }
                )
            return groups

        labels = self._connected_components(n, edges)

        # ë¼ë²¨ -> ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        label_to_indices: Dict[int, List[int]] = {}
        for idx, lbl in enumerate(labels):
            label_to_indices.setdefault(lbl, []).append(idx)

        groups: List[Dict] = []
        for label, idxs in label_to_indices.items():
            group_photos = [photo_files[i] for i in idxs]
            group_feats = features_array[idxs]

            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê³¼ì˜ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            centroid = group_feats.mean(axis=0)
            norm = np.linalg.norm(centroid)
            if norm > 0:
                centroid = centroid / norm
            sims = group_feats @ centroid
            avg_similarity = float(np.mean(sims))

            groups.append(
                {
                    "id": label,
                    "photos": group_photos,
                    "count": len(group_photos),
                    "avg_similarity": avg_similarity,
                    "quality_score": avg_similarity * len(group_photos),
                }
            )

        # í’ˆì§ˆ ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        groups.sort(key=lambda g: g["quality_score"], reverse=True)
        return groups

    # ------------------------------------------------------------------
    # íŒŒì´í”„ë¼ì¸ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
    # ------------------------------------------------------------------
    def cluster_photos(self):
        logger.info("ğŸ” Starting deep learning-based photo analysis...")
        photo_files = self.load_photos()
        if len(photo_files) < 2:
            logger.warning("âŒ Not enough images to analyze.")
            return

        features_list: List[np.ndarray] = []
        valid_photos: List[Path] = []

        logger.info("ğŸš€ Extracting high-dimensional features... (caching may speed this up)")
        start_time = time.time()
        for photo_file in tqdm(photo_files, desc="Extracting deep features"):
            combined_features, _features_dict = self.extract_deep_features(photo_file)
            if combined_features is not None:
                features_list.append(combined_features)
                valid_photos.append(photo_file)

        shapes = {f.shape for f in features_list}
        logger.info(f"Feature shapes: {shapes}")
        
        extraction_time = time.time() - start_time
        logger.info(
            f"âœ… Extracted features from {len(valid_photos)} images in {extraction_time:.1f}s"
        )

        if self.use_cache and hasattr(self, "cache_stats"):
            total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
            hit_rate = (
                self.cache_stats["hits"] / total_requests * 100 if total_requests > 0 else 0.0
            )
            logger.info(
                f"ğŸ’¾ Cache stats: {self.cache_stats['hits']} hits, "
                f"{self.cache_stats['misses']} misses ({hit_rate:.1f}% hit rate)"
            )

        if not features_list:
            logger.warning("âŒ No valid features extracted; aborting clustering.")
            return

        features_array = np.stack(features_list, axis=0)
        logger.info(f"ğŸ“Š Feature vector dimensions: {features_array.shape}")

        self.groups = self.advanced_clustering(features_array, valid_photos)
        logger.info(f"âœ… Clustered into {len(self.groups)} groups with high precision.")
        for group in self.groups:
            quality_desc = (
                "High"
                if group["quality_score"] > 2
                else "Medium"
                if group["quality_score"] > 1
                else "Low"
            )
            logger.info(
                f"   ğŸ“ Group {group['id']}: {group['count']} photos, "
                f"avg similarity: {group['avg_similarity']:.3f}, "
                f"Quality: {quality_desc}"
            )

    async def cluster(self, photo_paths: List[str]) -> List[List[str]]:
        """
        Clusters a given list of photo paths.
        This is the main entry point when used as part of a pipeline.
        (ê¸°ì¡´ deep_clusterer ì™€ ë™ì¼í•œ ì‹œê·¸ë‹ˆì²˜ ìœ ì§€)
        """
        self.photos = [Path(p.path) for p in photo_paths]
        self.cluster_photos()

        sub_clusters: List[List[str]] = []
        if self.groups:
            for group in self.groups:
                sub_clusters.append([str(photo) for photo in group["photos"]])
        return sub_clusters

    def run(self):
        """
        ì…ë ¥ í´ë”(self.input_path) ì „ì²´ë¥¼ ëŒ€ìƒìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§í•˜ëŠ” í¸ì˜ ë©”ì„œë“œ.
        """
        self.photos = []  # í´ë” ì „ì²´ ì‚¬ìš©
        self.create_output_folders()
        self.cluster_photos()
        self.copy_photos_to_groups()
        self.create_master_result_image()
        self.create_detailed_report()

    # ------------------------------------------------------------------
    # ê²°ê³¼ ì €ì¥ / ì‹œê°í™”
    # ------------------------------------------------------------------
    def create_output_folders(self):
        if self.output_path.exists():
            logger.info("ğŸ—‘ï¸ Deleting existing output folder...")
            shutil.rmtree(self.output_path)
        self.output_path.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ Created output folder: {self.output_path}")

    def copy_photos_to_groups(self):
        logger.info("ğŸ“‹ Copying photos to group folders...")
        for group in tqdm(self.groups, desc="Creating folders"):
            group_folder = self.output_path / f"location_{group['id']}"
            group_folder.mkdir(exist_ok=True)
            for photo_path in group["photos"]:
                shutil.copy2(photo_path, group_folder / photo_path.name)

    def create_master_result_image(self):
        logger.info("ğŸ¨ Creating master result image...")
        if not self.groups:
            return

        cell_width, cell_height, header_height, padding, cols = 300, 250, 40, 10, 3
        rows = (len(self.groups) + cols - 1) // cols
        canvas_width = cols * (cell_width + padding) - padding
        canvas_height = rows * (cell_height + header_height + padding) - padding
        master_image = Image.new("RGB", (canvas_width, canvas_height), "white")
        draw = ImageDraw.Draw(master_image)

        try:
            from PIL import ImageFont

            try:
                font_dir = Path("/app/fonts")
                font = ImageFont.truetype(str(font_dir / "AppleGothic.ttf"), 16)
            except Exception:
                font = ImageFont.load_default()
        except Exception:
            ImageFont = None  # type: ignore
            font = None  # type: ignore

        for idx, group in enumerate(self.groups):
            row = idx // cols
            col = idx % cols
            group_x = col * (cell_width + padding)
            group_y = row * (cell_height + header_height + padding)

            # í—¤ë” í…ìŠ¤íŠ¸
            header_text = (
                f"Group {group['id']}  "
                f"(n={group['count']}, "
                f"sim={group['avg_similarity']:.2f})"
            )
            if font is not None:
                draw.text((group_x + 5, group_y + 5), header_text, fill="black", font=font)
            else:
                draw.text((group_x + 5, group_y + 5), header_text, fill="black")

            # ì‚¬ì§„ë“¤(ìµœëŒ€ 3ì¥)
            photos_to_show = group["photos"][:3]
            photo_width = cell_width // 3
            for photo_idx, photo_path in enumerate(photos_to_show):
                try:
                    img = Image.open(photo_path).resize(
                        (photo_width - 2, cell_height - 2), Image.Resampling.LANCZOS
                    )
                    photo_x = group_x + photo_idx * photo_width + 1
                    photo_y = group_y + header_height + 1
                    master_image.paste(img, (photo_x, photo_y))
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to process photo {photo_path}: {e}")

        master_path = self.output_path / "classification_result.jpg"
        master_image.save(master_path, quality=95, optimize=True)
        logger.info(f"âœ… Master result image saved: {master_path}")
        return master_path

    def create_detailed_report(self):
        summary = {
            "analysis_info": {
                "model_used": ["OpenCLIP ViT-B-32", "EfficientNet-B4", "Vision Transformer"],
                "device": str(self.device),
                "similarity_threshold": self.similarity_threshold,
            },
            "total_photos": sum(g["count"] for g in self.groups),
            "total_groups": len(self.groups),
            "groups": [
                {
                    "id": str(g["id"]),
                    "photo_count": g["count"],
                    "average_similarity": float(g["avg_similarity"]),
                    "quality_score": float(g["quality_score"]),
                    "photos": [p.name for p in g["photos"]],
                }
                for g in self.groups
            ],
        }

        self.output_path.mkdir(parents=True, exist_ok=True)
        report_path = self.output_path / "analysis_report.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“Š Detailed report saved: {report_path}")
    
    def condition(self, c):
        return True