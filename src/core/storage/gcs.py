import asyncio
import logging
from datetime import timedelta
from pathlib import Path
from typing import BinaryIO, Optional
import google.auth
from google.cloud import storage  # 동기 라이브러리
# from google.oauth2 import service_account

from core.config import configs

from .base import StorageService

logger = logging.getLogger(__name__)


class GCSStorageService(StorageService):
    _client: Optional[storage.Client] = None

    def __init__(self):
        if GCSStorageService._client is None:
            logger.info("Initializing new GCS Client...")
            GCSStorageService._client = storage.Client()
        
        self.client = GCSStorageService._client
        self.bucket_name = configs.GCS_BUCKET_NAME
        self.bucket = self.client.bucket(self.bucket_name)

        logger.info(f"GCSStorageService initialized for bucket '{self.bucket_name}'")

    async def save_file(self, file: BinaryIO, path: str, content_type: str = None) -> str:
        blob = self.bucket.blob(path)

        # file to memory (file 객체가 비동기 read()를 지원해야 함)
        content = await file.read()

        def upload_sync():
            blob.upload_from_string(content, content_type=content_type)
            logger.info(f"[GCS] Uploaded to {path}")
            return path

        return await asyncio.to_thread(upload_sync)

    async def delete_file(self, path: str) -> bool:
        blob = self.bucket.blob(path)

        def delete_sync():
            if blob.exists():
                blob.delete()
                logger.info(f"[GCS] Deleted {path}")
                return True
            return False

        return await asyncio.to_thread(delete_sync)

    async def delete_directory(self, prefix: str) -> bool:
        if not prefix.endswith("/"):
            prefix += "/"

        def delete_directory_sync():
            blobs = list(self.bucket.list_blobs(prefix=prefix))

            if not blobs:
                logger.info(f"[GCS] Directory prefix '{prefix}' not found or is empty.")
                return False

            self.bucket.delete_blobs(blobs)
            deleted_names = [blob.name for blob in blobs]
            logger.info(f"[GCS] Deleted directory content ({len(deleted_names)} files) for prefix: {prefix}")
            return True

        return await asyncio.to_thread(delete_directory_sync)

    async def move_file(self, source_path: str, dest_path: str) -> str:
        source_blob = self.bucket.blob(source_path)

        def move_sync():
            if not source_blob.exists():
                raise FileNotFoundError(f"Source file not found: {source_path}")

            self.bucket.rename_blob(source_blob, dest_path)
            logger.info(f"[GCS] Moved {source_path} to {dest_path}")
            return dest_path

        return await asyncio.to_thread(move_sync)

    def get_url(self, path: str) -> str:
        if path.startswith("http://") or path.startswith("https://"):
            return path
        return f"https://storage.googleapis.com/{self.bucket_name}/{path}"

    def generate_upload_url(self, path: str, content_type: str = None) -> Optional[str]:
        blob = self.bucket.blob(path)

        url = blob.generate_signed_url(
            version="v4",
            expiration=timedelta(minutes=15),
            method="PUT",
            content_type=content_type,
        )
        return url

    def generate_resumable_session_url(self, target_path: str, content_type: str, origin: str = None) -> str:
        """
        GCS Resumable Upload Session을 시작하고,
        클라이언트가 업로드를 수행할 수 있는 Session URL을 반환합니다.
        """
        blob = self.bucket.blob(target_path)

        # 세션 시작 (백엔드에서 GCS로 요청을 보냄)
        # origin 인자는 CORS Preflight를 위해 클라이언트의 도메인을 넣어주는 것이 좋습니다.
        upload_url = blob.create_resumable_upload_session(content_type=content_type, origin=origin)

        return upload_url

    async def download_file(self, path: str, destination_local_path: Path):
        blob_name = path.replace(f"https://storage.googleapis.com/{self.bucket_name}/", "")
        blob = self.bucket.blob(blob_name)

        def download_sync():
            blob.download_to_filename(str(destination_local_path))
            logger.info(f"[GCS] Downloaded {path} to {destination_local_path}")

        await asyncio.to_thread(download_sync)

    def download_partial_bytes(self, path: str, end_byte: int = 50 * 1024) -> Optional[bytes]:
        """
        [최적화 핵심] 파일의 앞부분(0 ~ end_byte)만 메모리로 다운로드합니다.
        이미지 썸네일/헤더 추출용으로 사용됩니다.
        """
        blob_name = path.replace(f"https://storage.googleapis.com/{self.bucket_name}/", "")

        try:
            # 멀티프로세싱 환경에서는 이 메서드 내부에서 Client가 init 되는 것이 안전함
            # (self.client 프로퍼티가 호출 시점에 생성하므로 OK)
            # bucket_name, blob_name = self.parse_gcs_path(path)

            # 직접 bucket 객체 생성 (self.bucket 사용 시 의존성 문제 회피)
            blob = self.bucket.blob(blob_name)

            # range request (start=0, end=end_byte)
            return blob.download_as_bytes(start=0, end=end_byte)
        except Exception as e:
            logger.warning(f"[GCS] Partial download failed for {path}: {e}")
            return None

    async def list_files(self, prefix: str) -> list[str]:
        if not prefix.endswith("/"):
            prefix += "/"

        def list_sync():
            blobs = list(self.bucket.list_blobs(prefix=prefix))
            return [blob.name for blob in blobs if not blob.name.endswith("/")]  # Exclude directory marker

        return await asyncio.to_thread(list_sync)
